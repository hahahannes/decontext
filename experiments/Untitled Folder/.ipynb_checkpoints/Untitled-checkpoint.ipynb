{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os \n",
    "import sys \n",
    "sys.path.append('../../..')\n",
    "sys.path\n",
    "os.environ['DATA_DIR'] = '/home/hannes/data/laptop_sync/Uni/master/masterarbeit/decon/DecontextEmbeddings/data'\n",
    "\n",
    "from helpers.data import load_embedding_to_df\n",
    "from helpers.things_evaluation.evaluate import read_embeddings\n",
    "\n",
    "os.environ['EMBEDDING_EVALUATION_DATA_PATH'] = './embedding_evaluation/data/'\n",
    "figure_dir = '../../figures'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_things(df):\n",
    "    words = set(pd.read_csv('../../../../data/thinga/things_concepts.tsv', sep='\\t')['uniqueID'])\n",
    "    missing = words.difference(df.index)\n",
    "    print(missing)\n",
    "    print(f'{len(missing)} missing of {len(words)}')\n",
    "\n",
    "def stats_things(corpus_folder):\n",
    "    matchings = ['word', 'synset', 'main_word', 'concept_id']\n",
    "    dfs = []\n",
    "    for matching in matchings:\n",
    "        print(f'Matching: {matching}')\n",
    "        path = f'../../../../data/thinga/{corpus_folder}/decontext/bert-base/1/{matching}/mean/1/decontext.txt'\n",
    "        df = read_embeddings(path, matching, keep_n_contexts=True)[['n_contexts']]\n",
    "        df = df.sort_values(by='n_contexts', ascending=False)\n",
    "        dfs.append(df)\n",
    "        \n",
    "        total = df['n_contexts'].sum()\n",
    "        print(f'Total number of embeddings: {total}')\n",
    "        print(f'{df.shape[0]} concepts found')\n",
    "        #print(df['n_contexts'].describe())\n",
    "        missing_things(df)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "def get_hist(df_word, df_synset, df_main_word, df_concept_id):\n",
    "    fig, axes = plt.subplots(1,1, figsize=(5,5), sharey=False, sharex=True)\n",
    "    axes.hist(df_word['n_contexts'], bins=200)\n",
    "    #axes[1].hist(df_synset['n_contexts'], bins=200)\n",
    "    #axes[2].hist(df_main_word['n_contexts'], bins=200)\n",
    "    #axes[3].hist(df_concept_id['n_contexts'], bins=200)\n",
    "    axes.set_ylabel('number of words')\n",
    "    axes.set_xlabel('number of contexts per word')\n",
    "    #axes[0].set_title('per word')\n",
    "    #axes[1].set_title('per wordnet synset id')\n",
    "    #axes[2].set_title('per concept')\n",
    "    #axes[3].set_title('per wordnet concept id')\n",
    "\n",
    "def plot_boxplot(df_word, df_synset, df_main_word, df_concept_id):\n",
    "    fig, axes = plt.subplots(1,1, figsize=(20,5), sharey=True, sharex=True)\n",
    "    axes.boxplot([df_word['n_contexts'], df_synset['n_contexts'], df_main_word['n_contexts'], df_concept_id['n_contexts']], vert=False)\n",
    "    axes.set_yticks([1,2,3,4])\n",
    "    axes.set_yticklabels(['word', 'wordnet synset id', 'concept', 'concept wordnet id'])\n",
    "    axes.set_xlabel('number of contexts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word, df_synset, df_main_word, df_concept_id = stats_things('wikidumps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
