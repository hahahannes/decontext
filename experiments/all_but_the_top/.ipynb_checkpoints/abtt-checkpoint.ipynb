{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_prettify' from 'helpers.plot' (/home/hhansen/decon/decon_env/DecontextEmbeddings/helpers/plot.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-e68cb6679183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myield_static_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_intersection_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_style_and_font_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_prettify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mset_style_and_font_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'model_prettify' from 'helpers.plot' (/home/hhansen/decon/decon_env/DecontextEmbeddings/helpers/plot.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys \n",
    "sys.path.append('/home/hhansen/decon/decon_env/DecontextEmbeddings')\n",
    "import os \n",
    "EMBEDDING_DATA_DIR = '/home/hhansen/decon/decon_env/data'\n",
    "RETRAIN_EMBEDDING_DATA_DIR = '/home/hhansen/decon/decon_env/data_fine_tune'\n",
    "\n",
    "os.environ['EMBEDDING_DATA_DIR'] = EMBEDDING_DATA_DIR\n",
    "os.environ['EMBEDDING_EVALUATION_DATA_PATH'] = '/home/hhansen/decon/decon_env/DecontextEmbeddings/helpers/embedding_evaluation/data/'\n",
    "DATA_DIR = '/home/hhansen/decon/decon_env/DecontextEmbeddings/data'\n",
    "os.environ['DATA_DIR'] = DATA_DIR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from helpers.embedding_evaluation.evaluate import Evaluation as wordsim_evaluate\n",
    "from helpers.embedding_evaluation.data import read_wordsim_embeddings\n",
    "from helpers.things_evaluation.evaluate import read_embeddings, load_behav, load_sorting, match_behv_sim, evaluate as run_evaluation\n",
    "from helpers.data import yield_static_data\n",
    "from helpers.intersection import get_intersection_words\n",
    "from helpers.plot import get_ax, set_style_and_font_size, model_prettify\n",
    "\n",
    "set_style_and_font_size()\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from collections import defaultdict\n",
    "import os \n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(combs, results, results_anistropy_cleaned, title):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,5), sharey= True)\n",
    "\n",
    "    for i, _ in enumerate(combs.items()):\n",
    "        ax = axes[i]\n",
    "        model, n_context_mapping = _\n",
    "        for n_context, layers in n_context_mapping.items():\n",
    "            layers = [str(layer) for layer in layers]            \n",
    "            label = 'Decontextualized' if n_context == 'all' else 'Contextualized'\n",
    "            c = 'C0' if n_context == 'all' else 'C1'\n",
    "            ax.plot(layers, results[model][n_context], marker='o', label=label + ' - without postprocessing', linestyle='dotted', c=c)\n",
    "            ax.plot(layers, results_anistropy_cleaned[model][n_context], marker='o', label=label + ' - with postprocessing', c=c)\n",
    "\n",
    "        ax.set_xticks(layers)\n",
    "        \n",
    "        if 'w2v' in results_anistropy_cleaned.keys():\n",
    "            ax.axhline(results_anistropy_cleaned['w2v'], label='Word2Vec - with postprocessing', c='C3')\n",
    "            ax.axhline(results['w2v'], label='Word2Vec - without postprocessing', linestyle='dotted', c='C3')\n",
    "        \n",
    "        ax.set_title(f'{model_prettify(model)}')\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Extraction layer')\n",
    "    ax.set_ylabel('Spearman correlation')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'abtt_{title}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropy(embeddings):\n",
    "    embeddings_np = embeddings.to_numpy()\n",
    "\n",
    "    mean_normalized = embeddings_np - np.mean(embeddings_np, axis=0)\n",
    "\n",
    "    D = int(mean_normalized.shape[1] / 100)\n",
    "    u = PCA(n_components=D).fit(mean_normalized).components_ \n",
    "    \n",
    "    print(mean_normalized.shape)\n",
    "    print(u.shape)\n",
    "    \n",
    "    isotropy_embeddings = mean_normalized - (embeddings_np @ u.T @ u)  \n",
    "    \n",
    "    return pd.DataFrame(isotropy_embeddings, index=embeddings.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = {\n",
    "    'bert-base': {\n",
    "        '1': range(13),\n",
    "        'all': range(13)\n",
    "    },\n",
    "    'gpt-2': {\n",
    "        '1': range(13),\n",
    "        'all': range(13)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w2v', '/home/hhansen/decon/decon_env/data/thinga/static/w2v/word2vec-google-news-300/embeddings.txt', 'word')\n",
      "('glove', '/home/hhansen/decon/decon_env/data/thinga/static/glove/glove-wiki-gigaword-300/embeddings.txt', 'word')\n",
      "('deconf', '/home/hhansen/decon/decon_env/data/thinga/static/deconf/embeddings.txt', 'synset')\n"
     ]
    }
   ],
   "source": [
    "corpus = 'wikidumps'\n",
    "matching = 'word'\n",
    "results = defaultdict(lambda: defaultdict(list))\n",
    "results_anistropy_cleaned = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "matching_words_things = get_intersection_words(1, matching=matching, corpus_folder=corpus, folder='thinga')\n",
    "print(f'use {len(matching_words_things)} for things')\n",
    "   \n",
    "for model, embedding_path, static_matching in yield_static_data('thinga'):\n",
    "    df = read_embeddings(embedding_path, matching=static_matching, matching_words=matching_words_things)\n",
    "    pearson, spearman, matrix, vector = run_evaluation(df, matching=static_matching, matching_words=matching_words_things)\n",
    "    results[model] = spearman.correlation\n",
    "\n",
    "    pearson, spearman, matrix, vector = run_evaluation(isotropy(df), matching)\n",
    "    results_anistropy_cleaned[model] = spearman.correlation\n",
    "        \n",
    "for model, n_context_mapping in combs.items():\n",
    "    for n_context, layers in n_context_mapping.items():\n",
    "        for layer in layers:\n",
    "            path = f'{EMBEDDING_DATA_DIR}/thinga/{corpus}/decontext/{model}/{layer}/{matching}/mean/{n_context}/decontext.txt'\n",
    "            df = read_embeddings(path, matching=matching, matching_words=matching_words_things)\n",
    "            pearson, spearman, matrix, vector = run_evaluation(df, matching)\n",
    "            results[model][n_context].append(spearman.correlation)\n",
    "                \n",
    "            pearson, spearman, matrix, vector = run_evaluation(isotropy(df), matching)\n",
    "            results_anistropy_cleaned[model][n_context].append(spearman.correlation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(combs, results, results_anistropy_cleaned, 'things')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dict(df):\n",
    "    embedding_dict = {}\n",
    "    for row in df.iterrows():\n",
    "        word = row[0]\n",
    "        embedding = list(row[1])\n",
    "        embedding_dict[word] = embedding\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'wikidumps'\n",
    "matching = 'word'\n",
    "results = defaultdict(lambda: defaultdict(list))\n",
    "results_anistropy_cleaned = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "matching_words = get_intersection_words(1, matching=matching, corpus_folder=corpus, folder='word_sim')\n",
    "print(f'use {len(matching_words)} for wordsim')\n",
    "   \n",
    "for model, embedding_path, static_matching in yield_static_data('word_sim'):\n",
    "    print(model)\n",
    "    embeddings = read_wordsim_embeddings(embedding_path, matching_words, matching=static_matching, as_df=True)\n",
    "    evaluation = wordsim_evaluate() \n",
    "    eval_results = evaluation.evaluate(df_to_dict(embeddings))\n",
    "    spearman_simlex = eval_results['similarity']['simlex']['all_entities']\n",
    "    results[model] = spearman_simlex\n",
    "\n",
    "    eval_results = evaluation.evaluate(df_to_dict(isotropy(embeddings)))\n",
    "    spearman_simlex = eval_results['similarity']['simlex']['all_entities']\n",
    "    results_anistropy_cleaned[model] = spearman_simlex\n",
    "        \n",
    "for model, n_context_mapping in combs.items():\n",
    "    print(model)\n",
    "    for n_context, layers in n_context_mapping.items():\n",
    "        for layer in layers:\n",
    "            path = f'{EMBEDDING_DATA_DIR}/word_sim/{corpus}/decontext/{model}/{layer}/{matching}/mean/{n_context}/decontext.txt'\n",
    "            embeddings = read_wordsim_embeddings(path, matching_words, matching=matching, as_df=True)\n",
    "            evaluation = wordsim_evaluate() \n",
    "            eval_results = evaluation.evaluate(df_to_dict(embeddings))\n",
    "            spearman_simlex = eval_results['similarity']['simlex']['all_entities']\n",
    "            results[model][n_context].append(spearman_simlex)\n",
    "\n",
    "            eval_results = evaluation.evaluate(df_to_dict(isotropy(embeddings)))\n",
    "            spearman_simlex = eval_results['similarity']['simlex']['all_entities']\n",
    "            results_anistropy_cleaned[model][n_context].append(spearman_simlex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(combs, results, results_anistropy_cleaned, 'wordsim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS  - Retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_retrain(combs, results, results_anistropy_cleaned, title):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(5,5), sharey= True)\n",
    "\n",
    "    for i, _ in enumerate(combs.items()):\n",
    "        model, n_context_mapping = _\n",
    "        for n_context, layers in n_context_mapping.items():\n",
    "            layers = [str(layer) for layer in layers]            \n",
    "            c = 'C0' if n_context == 'all' else 'C1'\n",
    "            ax.plot(layers, results[model][n_context], marker='o', label='without postprocessing', linestyle='dotted', c=c)\n",
    "            ax.plot(layers, results_anistropy_cleaned[model][n_context], marker='o', label='with postprocessing', c=c)\n",
    "\n",
    "        ax.set_xticks(layers)\n",
    "        ax.set_title(f'{model_prettify(model)}')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Extraction layer')\n",
    "    ax.set_ylabel('Spearman correlation')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'abtt_{title}_retrain.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = {\n",
    "    'bert-base': {\n",
    "        'all': range(13)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'wikidumps'\n",
    "matching = 'word'\n",
    "retrain_results = defaultdict(lambda: defaultdict(list))\n",
    "retrain_results_anistropy_cleaned = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "matching_words_things = get_intersection_words(1, matching=matching, corpus_folder=corpus, folder='thinga')\n",
    "print(f'use {len(matching_words_things)} for things')\n",
    "        \n",
    "for model, n_context_mapping in combs.items():\n",
    "    for n_context, layers in n_context_mapping.items():\n",
    "        for layer in layers:\n",
    "            path = f'{RETRAIN_EMBEDDING_DATA_DIR}/things/{corpus}/decontext/{model}/{layer}/{matching}/mean/{n_context}/constrastive_loss.txt'\n",
    "            df = read_embeddings(path, matching=matching, matching_words=matching_words_things)\n",
    "            pearson, spearman, matrix, vector = run_evaluation(df, matching)\n",
    "            retrain_results[model][n_context].append(spearman.correlation)\n",
    "                \n",
    "            pearson, spearman, matrix, vector = run_evaluation(isotropy(df), matching)\n",
    "            retrain_results_anistropy_cleaned[model][n_context].append(spearman.correlation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_retrain(combs, retrain_results, retrain_results_anistropy_cleaned, 'things_retrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
